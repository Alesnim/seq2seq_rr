{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alesnim/seq2seq_rr/blob/master/preprocessing%20data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "37CJmHyJuB-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "num0 = list('111101101101111')\n",
        "num1 = list('001001001001001')\n",
        "num2 = list('111001111100111')\n",
        "num3 = list('111001111001111')\n",
        "num4 = list('101101111001001')\n",
        "num5 = list('111100111001111')\n",
        "num6 = list('111100111101111')\n",
        "num7 = list('111001001001001')\n",
        "num8 = list('111101111101111')\n",
        "num9 = list('111101111001111')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nJ-Hfa5JuY6g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nums = [num0, num1, num2, num3, num4, num5, num6, num7, num8, num9]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Odm7C-1pues3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Виды цифры 5 (Тестовая выборка)\n",
        "num51 = list('111100111000111')\n",
        "num52 = list('111100010001111')\n",
        "num53 = list('111100011001111')\n",
        "num54 = list('110100111001111')\n",
        "num55 = list('110100111001011')\n",
        "num56 = list('111100101001111')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_nfCGxEuh7W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights = [0 for i in range(15)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CkUDxIeDu1pW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bias = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zSrJ9iexu4eR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def aes_nn(num):\n",
        "  net = 0 \n",
        "  for i in range(15):\n",
        "    net += int(num[i])*weights[i]\n",
        "  return  net >= bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nj1G3ZmCvnQ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decrease(num):\n",
        "  for i in range(15):\n",
        "    if int(num[i]) == 1:\n",
        "      weights[i] -= 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YZUOl-eawg74",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def increase(num):\n",
        "  for i in range(15):\n",
        "    if int(num[i]) == 1:\n",
        "      weights[i] += 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hh1yV3ahxIJ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(100000):\n",
        "  options = random.randint(0,9)\n",
        "  \n",
        "  if options != 5: \n",
        "    if aes_nn(nums[options]):\n",
        "      decrease(nums[options])\n",
        "    else:\n",
        "      if not aes_nn(num5):\n",
        "        increase(num5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Na6zs6XMyKnY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2058cf5-ce94-49cb-d8b8-0ee1cc82fcf2"
      },
      "cell_type": "code",
      "source": [
        "print(weights)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 2, 0, -7, 1, 2, 1, -7, 0, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZKF1xG13y9m2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "21b49df9-253d-4565-8f0f-0d877af1aa05"
      },
      "cell_type": "code",
      "source": [
        "print(\"0 это 5? \", aes_nn(num0))\n",
        "print(\"1 это 5? \", aes_nn(num1))\n",
        "print(\"2 это 5? \", aes_nn(num2))\n",
        "print(\"3 это 5? \", aes_nn(num3))\n",
        "print(\"4 это 5? \", aes_nn(num4))\n",
        "print(\"6 это 5? \", aes_nn(num6))\n",
        "print(\"7 это 5? \", aes_nn(num7))\n",
        "print(\"8 это 5? \", aes_nn(num8))\n",
        "print(\"9 это 5? \", aes_nn(num9), '\\n')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 это 5?  False\n",
            "1 это 5?  False\n",
            "2 это 5?  False\n",
            "3 это 5?  False\n",
            "4 это 5?  False\n",
            "6 это 5?  False\n",
            "7 это 5?  False\n",
            "8 это 5?  False\n",
            "9 это 5?  False \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gm6fLmHHzB6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c7163e01-e4c2-411e-a1ea-3d96f5fa4cb9"
      },
      "cell_type": "code",
      "source": [
        "print(\"Узнал 5? \", aes_nn(num5))\n",
        "print(\"Узнал 5 - 1? \", aes_nn(num51))\n",
        "print(\"Узнал 5 - 2? \", aes_nn(num52))\n",
        "print(\"Узнал 5 - 3? \", aes_nn(num53))\n",
        "print(\"Узнал 5 - 4? \", aes_nn(num54))\n",
        "print(\"Узнал 5 - 5? \", aes_nn(num55))\n",
        "print(\"Узнал 5 - 6? \", aes_nn(num56))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Узнал 5?  True\n",
            "Узнал 5 - 1?  True\n",
            "Узнал 5 - 2?  True\n",
            "Узнал 5 - 3?  True\n",
            "Узнал 5 - 4?  True\n",
            "Узнал 5 - 5?  True\n",
            "Узнал 5 - 6?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fIkTLGwfzXzg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "k = random.uniform(-5,5)\n",
        "c = random.uniform(-5,5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HO7UR8NzA6Ks",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    1: 2,\n",
        "    2: 4.2,\n",
        "    2.5: 5,\n",
        "    3.8: 7.9,\n",
        "    4: 9,\n",
        "    6: 10.2,\n",
        "    6.6: 13,\n",
        "    7.2: 15.3,\n",
        "    8: 17.1,\n",
        "    8.5: 19.5\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xvS3UJ3CBARl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bc56a7d1-1049-46be-dedb-f0c7cce7d34f"
      },
      "cell_type": "code",
      "source": [
        "rate = 0.0001\n",
        "def proseed(x):\n",
        "  return x*k+c\n",
        "\n",
        "print('Начальная прямая: ', k, '* X + ', c)\n",
        "for i in range(100000):\n",
        "    x = random.choice(list(data.keys()))\n",
        "    true_result = data[x]\n",
        "    out = proseed(x)\n",
        "    delta = true_result - out\n",
        "    k += delta*rate*x\n",
        "    c += delta*rate\n",
        "print('Готовая прямая: ', k, '* X + ', c)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Начальная прямая:  2.1712603903306515 * X +  -0.519443802061758\n",
            "Готовая прямая:  2.1574849621663423 * X +  -0.4116615491563347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eg1F7gzuBuPD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de76bad0-4253-4bc7-dfcd-e9c68093a866"
      },
      "cell_type": "code",
      "source": [
        "print('Начальная прямая: ', k, '* X + ', c)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Начальная прямая:  2.1712603903306515 * X +  -0.519443802061758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zBicAygJB0Sd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c233b85-2a61-4890-8701-fb9c1ff663f4"
      },
      "cell_type": "code",
      "source": [
        "print('Готовая прямая: ', k, '* X + ', c)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Готовая прямая:  2.1712603903306515 * X +  -0.519443802061758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oH0DgFV9B2-q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7cUAzlAVGvsW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Say hello.\n",
        "hello = tf.constant('Hello, TensorWorld!')\n",
        "sess = tf.Session()\n",
        "\n",
        "# --> Hello, TensorWorld!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xe8_I6SOPwBy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "d2e7de69-4872-4898-fc5c-4ee0e39b32ce"
      },
      "cell_type": "code",
      "source": [
        "print sess.run(hello)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-1377ef83d041>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print sess.run(hello)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-WjeIGAC65HD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-IMMUDnE69gs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O9q4kAbMsi6B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5352dc6-6c65-4ec0-efff-564a8871a9bc"
      },
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.11.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "upZz6wylsucJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PAD = 0\n",
        "EOS = 1\n",
        "\n",
        "vocab_size = 10\n",
        "input_embedding_size = 20\n",
        "\n",
        "encoder_hidden_units = 20\n",
        "decoder_hidden_units = encoder_hidden_units * 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ej0ZYm7bsyLM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
        "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
        "\n",
        "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmJB7BaqtIkc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
        "\n",
        "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q3ejAf-rtNeS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RwISOZaVtSBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_cell = LSTMCell(encoder_hidden_units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nenEuOoTtXQd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "((encoder_fw_outputs,\n",
        "  encoder_bw_outputs),\n",
        " (encoder_fw_final_state,\n",
        "  encoder_bw_final_state)) = (\n",
        "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
        "                                    cell_bw=encoder_cell,\n",
        "                                    inputs=encoder_inputs_embedded,\n",
        "                                    sequence_length=encoder_inputs_length,\n",
        "                                    dtype=tf.float32, time_major=True)\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J0MeQRKstnF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35efed18-dc82-4ee1-e093-126a29596ad2"
      },
      "cell_type": "code",
      "source": [
        "encoder_fw_outputs"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "IF0SimhBtpQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60250aa5-3337-4421-b863-e192cb8ffb2b"
      },
      "cell_type": "code",
      "source": [
        "encoder_bw_outputs"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "LdrcMW5str4T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "14d7639d-111d-4d6d-e896-e3a93b1de19d"
      },
      "cell_type": "code",
      "source": [
        "encoder_fw_final_state"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_4:0' shape=(?, 20) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "p-veBZestt5S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
        "\n",
        "encoder_final_state_c = tf.concat(\n",
        "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
        "\n",
        "encoder_final_state_h = tf.concat(\n",
        "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
        "\n",
        "encoder_final_state = LSTMStateTuple(\n",
        "    c=encoder_final_state_c,\n",
        "    h=encoder_final_state_h\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4hQ0l9KuEx6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_cell = LSTMCell(decoder_hidden_units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RfbaqGHzuH2S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_cell = LSTMCell(decoder_hidden_units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GTLELbscuOjo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NV_-DY2wuQao",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_lengths = encoder_inputs_length + 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-J6iYTzMuVQn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
        "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0G5SNgqual5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert EOS == 1 and PAD == 0\n",
        "\n",
        "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
        "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
        "\n",
        "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
        "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ebt7nnRumNu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loop_fn_initial():\n",
        "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
        "    initial_input = eos_step_embedded\n",
        "    initial_cell_state = encoder_final_state\n",
        "    initial_cell_output = None\n",
        "    initial_loop_state = None  # we don't need to pass any additional information\n",
        "    return (initial_elements_finished,\n",
        "            initial_input,\n",
        "            initial_cell_state,\n",
        "            initial_cell_output,\n",
        "            initial_loop_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b1MOG6jxvpTO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
        "\n",
        "    def get_next_input():\n",
        "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
        "        prediction = tf.argmax(output_logits, axis=1)\n",
        "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
        "        return next_input\n",
        "    \n",
        "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
        "                                                  # defining if corresponding sequence has ended\n",
        "\n",
        "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
        "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
        "    state = previous_state\n",
        "    output = previous_output\n",
        "    loop_state = None\n",
        "\n",
        "    return (elements_finished, \n",
        "            input,\n",
        "            state,\n",
        "            output,\n",
        "            loop_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gtsoUBeVvxuk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
        "    if previous_state is None:    # time == 0\n",
        "        assert previous_output is None and previous_state is None\n",
        "        return loop_fn_initial()\n",
        "    else:\n",
        "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
        "\n",
        "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
        "decoder_outputs = decoder_outputs_ta.stack()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gtBwUHUmv3px",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
        "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
        "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
        "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xLjVYz3Bv7fD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_prediction = tf.argmax(decoder_logits, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WkOnellev_rd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
        "    logits=decoder_logits,\n",
        ")\n",
        "\n",
        "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
        "train_op = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cTQUMGfVwGTn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PhbesxJfwQdF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_sequences(length_from, length_to,\n",
        "                     vocab_lower, vocab_upper,\n",
        "                     batch_size):\n",
        "    \"\"\" Generates batches of random integer sequences,\n",
        "        sequence length in [length_from, length_to],\n",
        "        vocabulary in [vocab_lower, vocab_upper]\n",
        "    \"\"\"\n",
        "    if length_from > length_to:\n",
        "            raise ValueError('length_from > length_to')\n",
        "\n",
        "    def random_length():\n",
        "        if length_from == length_to:\n",
        "            return length_from\n",
        "        return np.random.randint(length_from, length_to + 1)\n",
        "    \n",
        "    while True:\n",
        "        yield [\n",
        "            np.random.randint(low=vocab_lower,\n",
        "                              high=vocab_upper,\n",
        "                              size=random_length()).tolist()\n",
        "            for _ in range(batch_size)\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J1MrgbTGwVhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b8afc841-bcc1-4d0b-f874-0796527b997a"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "batches = random_sequences(length_from=3, length_to=8,\n",
        "                                   vocab_lower=2, vocab_upper=10,\n",
        "                                   batch_size=batch_size)\n",
        "\n",
        "print('head of the batch:')\n",
        "for seq in next(batches)[:10]:\n",
        "    print(seq)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "head of the batch:\n",
            "[8, 8, 5]\n",
            "[9, 4, 9, 5, 6, 5, 9, 3]\n",
            "[4, 2, 9, 4, 3, 8, 7, 9]\n",
            "[7, 9, 9, 8, 6]\n",
            "[8, 4, 6, 2, 9]\n",
            "[9, 6, 6]\n",
            "[3, 4, 5]\n",
            "[9, 8, 9, 7, 9, 6, 6]\n",
            "[7, 6, 4]\n",
            "[5, 4, 2, 8, 8, 2, 6, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G2FsRMLmwkGr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_feed():\n",
        "    batch = next(batches)\n",
        "    encoder_inputs_, encoder_input_lengths_ = batch_1(batch)\n",
        "    decoder_targets_, _ = batch_1(\n",
        "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
        "    )\n",
        "    return {\n",
        "        encoder_inputs: encoder_inputs_,\n",
        "        encoder_inputs_length: encoder_input_lengths_,\n",
        "        decoder_targets: decoder_targets_,\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5LzWq5o1x5Z3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_track = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IwZbgV-kx_vh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_1(inputs, max_sequence_length=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        inputs:\n",
        "            list of sentences (integer lists)\n",
        "        max_sequence_length:\n",
        "            integer specifying how large should `max_time` dimension be.\n",
        "            If None, maximum sequence length would be used\n",
        "    \n",
        "    Outputs:\n",
        "        inputs_time_major:\n",
        "            input sentences transformed into time-major matrix \n",
        "            (shape [max_time, batch_size]) padded with 0s\n",
        "        sequence_lengths:\n",
        "            batch-sized list of integers specifying amount of active \n",
        "            time steps in each input sequence\n",
        "    \"\"\"\n",
        "    \n",
        "    sequence_lengths = [len(seq) for seq in inputs]\n",
        "    batch_size = len(inputs)\n",
        "    \n",
        "    if max_sequence_length is None:\n",
        "        max_sequence_length = max(sequence_lengths)\n",
        "    \n",
        "    inputs_batch_major = np.zeros(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
        "    \n",
        "    for i, seq in enumerate(inputs):\n",
        "        for j, element in enumerate(seq):\n",
        "            inputs_batch_major[i, j] = element\n",
        "\n",
        "    # [batch_size, max_time] -> [max_time, batch_size]\n",
        "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
        "\n",
        "    return inputs_time_major, sequence_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zO5dImRwyCRV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "3080fbc1-a005-4132-e602-10d41b761435"
      },
      "cell_type": "code",
      "source": [
        "max_batches = 3001\n",
        "batches_in_epoch = 1000\n",
        "\n",
        "try:\n",
        "    for batch in range(max_batches):\n",
        "        fd = next_feed()\n",
        "        _, l = sess.run([train_op, loss], fd)\n",
        "        loss_track.append(l)\n",
        "\n",
        "        if batch == 0 or batch % batches_in_epoch == 0:\n",
        "            print('batch {}'.format(batch))\n",
        "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
        "            predict_ = sess.run(decoder_prediction, fd)\n",
        "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
        "                print('  sample {}:'.format(i + 1))\n",
        "                print('    input     > {}'.format(inp))\n",
        "                print('    predicted > {}'.format(pred))\n",
        "                if i >= 2:\n",
        "                    break\n",
        "            print()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('training interrupted')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch 0\n",
            "  minibatch loss: 2.293344497680664\n",
            "  sample 1:\n",
            "    input     > [9 5 3 5 4 0 0 0]\n",
            "    predicted > [8 4 6 2 2 3 3 3 0 0 0]\n",
            "  sample 2:\n",
            "    input     > [6 3 8 2 4 6 8 0]\n",
            "    predicted > [8 4 2 2 8 1 2 3 3 3 0]\n",
            "  sample 3:\n",
            "    input     > [6 2 2 5 8 2 0 0]\n",
            "    predicted > [5 4 9 3 3 3 3 3 3 0 0]\n",
            "\n",
            "batch 1000\n",
            "  minibatch loss: 0.5178948044776917\n",
            "  sample 1:\n",
            "    input     > [9 6 8 6 9 6 5 3]\n",
            "    predicted > [6 9 6 6 6 6 3 3 1 0 0]\n",
            "  sample 2:\n",
            "    input     > [6 6 2 8 7 4 0 0]\n",
            "    predicted > [6 6 8 2 7 4 1 0 0 0 0]\n",
            "  sample 3:\n",
            "    input     > [4 6 3 5 3 2 8 0]\n",
            "    predicted > [4 6 3 3 3 8 8 1 0 0 0]\n",
            "\n",
            "batch 2000\n",
            "  minibatch loss: 0.2301717847585678\n",
            "  sample 1:\n",
            "    input     > [8 8 8 0 0 0 0 0]\n",
            "    predicted > [8 8 8 1 0 0 0 0 0 0 0]\n",
            "  sample 2:\n",
            "    input     > [7 6 4 2 3 6 0 0]\n",
            "    predicted > [7 6 4 2 3 6 1 0 0 0 0]\n",
            "  sample 3:\n",
            "    input     > [3 4 2 9 8 4 2 0]\n",
            "    predicted > [3 4 2 9 8 2 2 1 0 0 0]\n",
            "\n",
            "batch 3000\n",
            "  minibatch loss: 0.15943557024002075\n",
            "  sample 1:\n",
            "    input     > [4 6 7 5 3 7 3 7]\n",
            "    predicted > [4 6 7 5 3 3 7 7 1 0 0]\n",
            "  sample 2:\n",
            "    input     > [2 2 5 2 3 0 0 0]\n",
            "    predicted > [2 2 5 2 3 1 0 0 0 0 0]\n",
            "  sample 3:\n",
            "    input     > [3 9 2 3 9 4 8 7]\n",
            "    predicted > [3 9 2 9 3 4 8 7 1 0 0]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eaHEsrN_yZQ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "bded09b6-157b-4951-e852-0de16c7131c7"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_track)\n",
        "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 0.1606 after 300100 examples (batch_size=100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0fca821828>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ErMQiEQiy-q0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e68fd3d9-25b3-4b1e-cfa9-81063885898e"
      },
      "cell_type": "code",
      "source": [
        "%% ls -a "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "en-SpDHrBRGR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d947102c-8ef4-4b4f-f368-10a1e59a6bd4"
      },
      "cell_type": "code",
      "source": [
        "% ls -a"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  \u001b[01;34m.config\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b8BFZnRvBT-q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2137841f-e583-483f-edc2-a488e830cda2"
      },
      "cell_type": "code",
      "source": [
        "% ls ./sample_data/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;32manscombe.json\u001b[0m*                mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  \u001b[01;32mREADME.md\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L_KXYfqxBZhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36825337-4c7b-4101-e0a4-f90126c60678"
      },
      "cell_type": "code",
      "source": [
        "% git clone https://github.com/Koziev/NLP_Datasets.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%git` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nzEvzws-IyeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "387fffa3-ccf3-411c-f808-b31980c8814c"
      },
      "cell_type": "code",
      "source": [
        "% apt-get install git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%apt` not found.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "EfZtiQ8FI7-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "02336d5d-621a-41d1-faf4-208d188040f2"
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Koziev/NLP_Datasets.git\n",
        " "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP_Datasets'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 323 (delta 0), reused 3 (delta 0), pack-reused 318\u001b[K\n",
            "Receiving objects: 100% (323/323), 169.10 MiB | 27.50 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n",
            "Checking out files: 100% (79/79), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aQ56v1H6I_Xf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33a0237c-abc7-476c-e9b4-b83e430f0a26"
      },
      "cell_type": "code",
      "source": [
        "% ls -a"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/  \u001b[01;34m..\u001b[0m/  \u001b[01;34m.config\u001b[0m/  \u001b[01;34mNLP_Datasets\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1M1VeBQGJHV9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e16910ac-327a-4c10-918c-d422ea7f9704"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Activation"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "STw0YyNGKjhi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "21dd1045-5af8-4cf7-e756-15528e62df92"
      },
      "cell_type": "code",
      "source": [
        "! ls ./NLP_Datasets -a"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\t       .git\t\t    QA\t       WordformFrequencies\n",
            "..\t       Lemmas\t\t    README.md  WordSubsets\n",
            "ChangePerson   MutualInfo\t    Samples\n",
            "Conversations  ParaphraseDetection  Stress\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C4H67Js6hTGS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e90871e3-c7a0-4db5-8ebd-e8899275c02b"
      },
      "cell_type": "code",
      "source": [
        "! ls ./NLP_Datasets/Conversations/"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ta6agaG2jvpY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "813936bc-3d25-4639-f9bb-499537c28598"
      },
      "cell_type": "code",
      "source": [
        "! tail ./NLP_Datasets/Conversations/Data/ru.conversations.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Ceкpeт.\n",
            "\n",
            "- Cмотpитe, завтpа вeтpeный бyдeт дeнь!\n",
            "- Oткyда вы знаeтe?\n",
            "- Видитe вокpyг лyны матовоe пятно?\n",
            "\n",
            "- Вы что, тожe читаeтe этот pоман?\n",
            "- A как жe... Kаждоe yтpо...\n",
            "- Чeгo-тo мeлкиe oни у вac...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YbBJM6O7lQzi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NOTjFIYZvh9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "433b15bc-023d-496f-8bf5-e00f7f7a37cb"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/suriyadeepan/practical_seq2seq.git\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'practical_seq2seq'...\n",
            "remote: Enumerating objects: 167, done.\u001b[K\n",
            "Receiving objects:   0% (1/167)   \rReceiving objects:   1% (2/167)   \rReceiving objects:   2% (4/167)   \rReceiving objects:   3% (6/167)   \rReceiving objects:   4% (7/167)   \rReceiving objects:   5% (9/167)   \rReceiving objects:   6% (11/167)   \rReceiving objects:   7% (12/167)   \rReceiving objects:   8% (14/167)   \rReceiving objects:   9% (16/167)   \rReceiving objects:  10% (17/167)   \rReceiving objects:  11% (19/167)   \rReceiving objects:  12% (21/167)   \rReceiving objects:  13% (22/167)   \rReceiving objects:  14% (24/167)   \rReceiving objects:  15% (26/167)   \rReceiving objects:  16% (27/167)   \rReceiving objects:  17% (29/167)   \rReceiving objects:  18% (31/167)   \rReceiving objects:  19% (32/167)   \rReceiving objects:  20% (34/167)   \rReceiving objects:  21% (36/167)   \rReceiving objects:  22% (37/167)   \rReceiving objects:  23% (39/167)   \rReceiving objects:  24% (41/167)   \rReceiving objects:  25% (42/167)   \rReceiving objects:  26% (44/167)   \rReceiving objects:  27% (46/167)   \rReceiving objects:  28% (47/167)   \rReceiving objects:  29% (49/167)   \rReceiving objects:  30% (51/167)   \rReceiving objects:  31% (52/167)   \rReceiving objects:  32% (54/167)   \rReceiving objects:  33% (56/167)   \rReceiving objects:  34% (57/167)   \rReceiving objects:  35% (59/167)   \rReceiving objects:  36% (61/167)   \rReceiving objects:  37% (62/167)   \rReceiving objects:  38% (64/167)   \rReceiving objects:  39% (66/167)   \rReceiving objects:  40% (67/167)   \rReceiving objects:  41% (69/167)   \rReceiving objects:  42% (71/167)   \rReceiving objects:  43% (72/167)   \rReceiving objects:  44% (74/167)   \rReceiving objects:  45% (76/167)   \rReceiving objects:  46% (77/167)   \rReceiving objects:  47% (79/167)   \rReceiving objects:  48% (81/167)   \rReceiving objects:  49% (82/167)   \rReceiving objects:  50% (84/167)   \rReceiving objects:  51% (86/167)   \rReceiving objects:  52% (87/167)   \rReceiving objects:  53% (89/167)   \rReceiving objects:  54% (91/167)   \rReceiving objects:  55% (92/167)   \rReceiving objects:  56% (94/167)   \rReceiving objects:  57% (96/167)   \rReceiving objects:  58% (97/167)   \rReceiving objects:  59% (99/167)   \rReceiving objects:  60% (101/167)   \rReceiving objects:  61% (102/167)   \rReceiving objects:  62% (104/167)   \rReceiving objects:  63% (106/167)   \rReceiving objects:  64% (107/167)   \rReceiving objects:  65% (109/167)   \rReceiving objects:  66% (111/167)   \rReceiving objects:  67% (112/167)   \rReceiving objects:  68% (114/167)   \rReceiving objects:  69% (116/167)   \rReceiving objects:  70% (117/167)   \rReceiving objects:  71% (119/167)   \rReceiving objects:  72% (121/167)   \rremote: Total 167 (delta 0), reused 0 (delta 0), pack-reused 167\u001b[K\n",
            "Receiving objects:  73% (122/167)   \rReceiving objects:  74% (124/167)   \rReceiving objects:  75% (126/167)   \rReceiving objects:  76% (127/167)   \rReceiving objects:  77% (129/167)   \rReceiving objects:  78% (131/167)   \rReceiving objects:  79% (132/167)   \rReceiving objects:  80% (134/167)   \rReceiving objects:  81% (136/167)   \rReceiving objects:  82% (137/167)   \rReceiving objects:  83% (139/167)   \rReceiving objects:  84% (141/167)   \rReceiving objects:  85% (142/167)   \rReceiving objects:  86% (144/167)   \rReceiving objects:  87% (146/167)   \rReceiving objects:  88% (147/167)   \rReceiving objects:  89% (149/167)   \rReceiving objects:  90% (151/167)   \rReceiving objects:  91% (152/167)   \rReceiving objects:  92% (154/167)   \rReceiving objects:  93% (156/167)   \rReceiving objects:  94% (157/167)   \rReceiving objects:  95% (159/167)   \rReceiving objects:  96% (161/167)   \rReceiving objects:  97% (162/167)   \rReceiving objects:  98% (164/167)   \rReceiving objects:  99% (166/167)   \rReceiving objects: 100% (167/167)   \rReceiving objects: 100% (167/167), 7.13 MiB | 24.84 MiB/s, done.\n",
            "Resolving deltas:   0% (0/73)   \rResolving deltas:   2% (2/73)   \rResolving deltas:   4% (3/73)   \rResolving deltas:   6% (5/73)   \rResolving deltas:  23% (17/73)   \rResolving deltas:  27% (20/73)   \rResolving deltas:  30% (22/73)   \rResolving deltas:  31% (23/73)   \rResolving deltas:  32% (24/73)   \rResolving deltas:  46% (34/73)   \rResolving deltas:  58% (43/73)   \rResolving deltas:  60% (44/73)   \rResolving deltas:  80% (59/73)   \rResolving deltas:  94% (69/73)   \rResolving deltas:  95% (70/73)   \rResolving deltas: 100% (73/73)   \rResolving deltas: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xEiXozXnwC4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd274d60-ec66-4437-d172-3936ed72bfb2"
      },
      "cell_type": "code",
      "source": [
        "! ls -a "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".  ..  .config\tNLP_Datasets  practical_seq2seq  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fW6BAwm1kHsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "98fa867c-2746-426a-9b19-5728a67de7b0"
      },
      "cell_type": "code",
      "source": [
        "! ls ./practical_seq2seq/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01-Phonemes-to-words.ipynb\t   datasets\n",
            "03-Twitter-chatbot.ipynb\t   data_utils.py\n",
            "03-Twitter-chatbot.py\t\t   img\n",
            "04-Cornell-Movie-Dialog-Bot.ipynb  LICENSE\n",
            "04-Cornell-Movie-Dialog-Bot.py\t   README.markdown\n",
            "ckpt\t\t\t\t   seq2seq_wrapper.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uWPjLNaQkMZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "76e95a1f-f016-426a-9f47-5a6d69dc45df"
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/llSourcell/tensorflow_chatbot.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tensorflow_chatbot'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "Unpacking objects:   1% (1/74)   \rUnpacking objects:   2% (2/74)   \rUnpacking objects:   4% (3/74)   \rUnpacking objects:   5% (4/74)   \rUnpacking objects:   6% (5/74)   \rUnpacking objects:   8% (6/74)   \rUnpacking objects:   9% (7/74)   \rUnpacking objects:  10% (8/74)   \rUnpacking objects:  12% (9/74)   \rUnpacking objects:  13% (10/74)   \rUnpacking objects:  14% (11/74)   \rUnpacking objects:  16% (12/74)   \rUnpacking objects:  17% (13/74)   \rUnpacking objects:  18% (14/74)   \rUnpacking objects:  20% (15/74)   \rUnpacking objects:  21% (16/74)   \rUnpacking objects:  22% (17/74)   \rUnpacking objects:  24% (18/74)   \rUnpacking objects:  25% (19/74)   \rUnpacking objects:  27% (20/74)   \rUnpacking objects:  28% (21/74)   \rUnpacking objects:  29% (22/74)   \rUnpacking objects:  31% (23/74)   \rUnpacking objects:  32% (24/74)   \rUnpacking objects:  33% (25/74)   \rUnpacking objects:  35% (26/74)   \rUnpacking objects:  36% (27/74)   \rUnpacking objects:  37% (28/74)   \rUnpacking objects:  39% (29/74)   \rUnpacking objects:  40% (30/74)   \rUnpacking objects:  41% (31/74)   \rUnpacking objects:  43% (32/74)   \rUnpacking objects:  44% (33/74)   \rUnpacking objects:  45% (34/74)   \rUnpacking objects:  47% (35/74)   \rUnpacking objects:  48% (36/74)   \rUnpacking objects:  50% (37/74)   \rUnpacking objects:  51% (38/74)   \rUnpacking objects:  52% (39/74)   \rUnpacking objects:  54% (40/74)   \rUnpacking objects:  55% (41/74)   \rUnpacking objects:  56% (42/74)   \rUnpacking objects:  58% (43/74)   \rUnpacking objects:  59% (44/74)   \rUnpacking objects:  60% (45/74)   \rUnpacking objects:  62% (46/74)   \rUnpacking objects:  63% (47/74)   \rUnpacking objects:  64% (48/74)   \rUnpacking objects:  66% (49/74)   \rUnpacking objects:  67% (50/74)   \rUnpacking objects:  68% (51/74)   \rUnpacking objects:  70% (52/74)   \rUnpacking objects:  71% (53/74)   \rUnpacking objects:  72% (54/74)   \rUnpacking objects:  74% (55/74)   \rUnpacking objects:  75% (56/74)   \rUnpacking objects:  77% (57/74)   \rUnpacking objects:  78% (58/74)   \rUnpacking objects:  79% (59/74)   \rUnpacking objects:  81% (60/74)   \rUnpacking objects:  82% (61/74)   \rUnpacking objects:  83% (62/74)   \rUnpacking objects:  85% (63/74)   \rUnpacking objects:  86% (64/74)   \rUnpacking objects:  87% (65/74)   \rUnpacking objects:  89% (66/74)   \rUnpacking objects:  90% (67/74)   \rUnpacking objects:  91% (68/74)   \rUnpacking objects:  93% (69/74)   \rUnpacking objects:  94% (70/74)   \rUnpacking objects:  95% (71/74)   \rUnpacking objects:  97% (72/74)   \rUnpacking objects:  98% (73/74)   \rremote: Total 74 (delta 0), reused 0 (delta 0), pack-reused 74\u001b[K\n",
            "Unpacking objects: 100% (74/74)   \rUnpacking objects: 100% (74/74), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ufCWNzahlTlC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b56154a-2fce-4819-e9f2-5c2bfb98ae5b"
      },
      "cell_type": "code",
      "source": [
        "! ls -a "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".   .config\t  practical_seq2seq  tensorflow_chatbot\n",
            "..  NLP_Datasets  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5oP0LvkYmPL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b49a21d3-6f57-4ab1-e42e-f816c5e64543"
      },
      "cell_type": "code",
      "source": [
        "! ls -a ./tensorflow_chatbot/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\t       execute.py   neuralconvo.ini   seq2seq_serve.ini\n",
            "..\t       .git\t    README.md\t      ui\n",
            "data_utils.py  .gitignore   seq2seq.ini       working_dir\n",
            "env_setup.sh   __init__.py  seq2seq_model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K_aqPWvMmXME",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! cp ./tensorflow_chatbot/data_utils.py ./data_utils.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_p1Ur6JfmxTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8d68c083-3649-4e8d-edf5-4b2d02c6b71e"
      },
      "cell_type": "code",
      "source": [
        "ls -a"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/   \u001b[01;34m.config\u001b[0m/  \u001b[01;32mdata_utils.py\u001b[0m*  \u001b[01;34mpractical_seq2seq\u001b[0m/  \u001b[01;34mtensorflow_chatbot\u001b[0m/\n",
            "\u001b[01;34m..\u001b[0m/  \u001b[01;34mdata\u001b[0m/     \u001b[01;34mNLP_Datasets\u001b[0m/   \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T9jSYOhomzLC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import dateutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EC8xaomum6ow",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import importlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zU3ibXVOnCSY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! mkdir data && cp ./data_utils.py ./data/data_utils.py && cp ./NLP_Datasets/Conversations/Data/ru.conversations.txt ./data/convers.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmBbt_iVnWk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01165f70-5d74-4557-e8cb-55de7310cd7e"
      },
      "cell_type": "code",
      "source": [
        "! ls -a ./data"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".  ..  convers.txt  data_utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UqmgfkUtpbST",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd ./data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2PuF6253POw3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! python3 ./data/data_utils.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ty9nT5JhPWpx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "159f5d13-e3c2-4cc2-b271-68e782e12ccd"
      },
      "cell_type": "code",
      "source": [
        "! ls -a ./data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".  ..  convers.txt  data_utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "092Ntq9GQNo1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! gzip ./data/convers.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5fEDJARKQTKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecd730a5-9821-4dd2-91d6-680a2e3b9f46"
      },
      "cell_type": "code",
      "source": [
        "! ls -a ./data"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".  ..  convers.txt.gz  data_utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "165OzTWjS7HV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! python3 ./data/data_utils.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MSYCqDNVTFoQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9205dda4-9061-4dc6-98b4-0f7586480efd"
      },
      "cell_type": "code",
      "source": [
        "! ls ./data"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convers.txt  data_utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sk0euLyRTS7M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f12b542-260e-436f-a49e-31dd246bd3a7"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "DIR = './data'\n",
        "file = os.listdir(DIR)\n",
        "reTag = re.compile('<.*?>')\n",
        "BAD_SYMBOLS = {'’', '≈', '√', '¬', '€', '“', '”', '∆', '„', '–',}\n",
        "\n",
        "print (file)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['convers.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "np2h3B46Vq4p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cleaned(text):\n",
        "    utterances = []\n",
        "    for p in text.split('\\n\\n'):\n",
        "        p = p.strip().split('\\n')\n",
        "        if len(p) < 3:\n",
        "            continue\n",
        "        p = reTag.sub('', ' '.join(p[2:]).strip()).replace('ƒ', 'д').replace('ќ', 'о')\n",
        "        if any(i in p for i in BAD_SYMBOLS):\n",
        "            continue\n",
        "        if p:\n",
        "            utterances.append(p)\n",
        "    return utterances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-7QzysmzWBsK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f92d84db-9e2e-4ab6-9a54-560f2fa1dc93"
      },
      "cell_type": "code",
      "source": [
        "fname = os.path.join(DIR, file[0])\n",
        "print(fname)\n",
        "with open(fname, 'r', encoding='utf-8') as f:\n",
        "    subs = cleaned(f.read().lower())\n",
        "print(subs[:150])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./data/convers.txt\n",
            "['- вези дальше. тут тысяча двадцать шестой.', '- значит, наша! принимай тяжелораненого!', '- а мы с тобой спим?!', '- ну тогда, адам, мы с вами сойдемся быстро.', '- это вы увидите сами.', '- во-первых, не \"ага\", а \"так точно\"... - виноват,  товарищ  капитан. - во-вторых, какой вид спорта? - легкая атлетика, товарищ капитан. - точнее? - стометровка, прыжки в высоту и диск.', '- пожалуй...', '- ленинград - моя родина.', '- правильно решил. молодец!', '- зачем она вам, девчонка эта... - вот  что,  загрудный.', '- прыгающие сымал? - приходилось и прыгающие...', '- да, выходим. выверни карманы!', '- да.', '- по росту последний во взводе! - в хвосте, значит? - на фланге, товарищ генерал!', '- танколовушки... разве вы не слышали? - слышал.', '- как это? - был в окружении, но не в плену. - но, позвольте, записано!... - в том и беда, что записано. - тогда почему не оспариваете? - устал, извините.', '- это хорошо!...', '- лейтенант под танком!... - де?...', '- ложись! ложись, дурак чертов, убьют!', '- ладно, \"повезло\"... набивай диски!', '- рассчитываю, когда \"юнкерсы\" снова пожалуют. - пожалуют еще... набивай диски!', '- ну...', '- тогда почему?... - провод изорвало в клочья, концов не найдешь! - есть запасная катушка. - тоже богу душу отдала...', '- звездочка...', '- хм...', '- в штабе фронта. я только от него. - от еременко? - да. - никак друзья? - знакомы. мы его прошлой осенью спасали...', '- когда?! - сегодня. сейчас не вернулся... - кто за вас будет докладывать? - лычкин вел первую группу, я - вторую. - что наблюдали? - \"мессеров\"  наблюдал,  товарищ  полковник.', '- поворачиваться надо. - а,  егошин?!', '- а котлубань? - котлубань не исключена.', '- сижу  без масла. - кто доставит? откуда? - из малой россошки...', '- моторное масло не подойдет? - нет.', '- дивизия молчит? раздаев? - раздаев. - обеспечение? - горючка доставлена, боезапас на подходе. - что значит \"на подходе\"? - отгрузили, везут... - сжатый воздух? - компрессора нет... - и компрессора нет!', '- интересно. нравится.', '- в  родном  доме  стены  греют.', '- технику нашу?', '- садись,  пока  свободен.', '- что  в  хозяйстве? - я соскучилась.', '- в авиации - да. - не  расстояние. - какие новости еще? - все празднуют, звание отмечают.', '- икар. - а первым штурманом? капитан молчал. - матрос железняк был первым штурманом. - почему железняк?', '- что? с кем?', '- минутку... - место  дай. место! - все разлилось, видишь... - вижу,  места  не  знаю. - потоп... уцепиться не за что...', '- дистанция огня?', '- а-а... я думал...', '- зачем? зачем? разболокайся, говорю...', '-  зде-е-еся!', '- баню затопляй! - середь ночи? -  середь  ночи.  экой барин! робенок-то!', '- смотрите, а в самолете ни одной пробоины!..', '- полетели вдвоем на ути-4?', '- кто тебя посылал туда?', '- откуда вы? - прямиком из преисподней,- смеется мажирин.', '- связались бы через штаб фронта. - сейчас как раз это делаем.', '- вызывайте еще.', '- очень даже хорошо! лампасы у него на брюках!', '- доволен, товарищ командующий.', '- как  прикажете,  товарищ  генерал. - мы  вас  очень просим, пане генерале.', '- ну, с благополучным исходом вас.', '- здесь только советские военнопленные? - да, русские, советские.', '- поехала б, да нельзя: служба!', '- наш. подбит недавно...', '- прикройсь!', '- да-а...сейчас эти приедут. тебе охота ехать? - не-е. - а чего едешь?', '- ты видела, куда он ушел? - видела... ничего я не видела. - внизу, в рецепции взять можно.', '- значит нп, говоришь?', '- почему не отвечали?', '- ладно,- сказал бабин.- вольно!', '- василий  кириллович!  двигай  к черевиченко!', '- завтра ко мне в торопу!', '- давно в армии, товарищ волков? - пятый год... - ну? а я думал, вы моложе! - что  вы,  товарищ  командующий.', '- кого взял? - унтера.', '- с того берега... мост смотрел. - когда восстановишь? - ночью будет готов.', '- бригадный комиссар и писатели...', '- нет, не очень, метров четыреста.', '- зайдите.', '- добавь скорость!', '- ну вот и хорошо. завтра накроем огнем.', '- лучше, пожалуй, не придумаешь.', '- я спрашиваю, почему они не срабатывают?', '- суханов  не  оступится.', '- второй день, товарищ командующий. - а где заслужили орден? - под чжалайнором. в двадцать девятом. - понятно.', '- огонь!', '- демонстративная атака? - это  -  как выйдет.', '- покормил? - вволю. есть боевая задача? - есть.', '- товарищ сержант алексеев. - позовите! - есть!', '- нехорошо. а еще гвардеец!', '- туда. отбивать крюково.', '- с десяток. и машины с пехотой. - что делают? - стоят на льду реки. моторы работают.', '- все  верно.', '- зовите!', '- к романову.', '- обмороженные? - нет.', '- не имею.', '- простяков! мне приказано принять дивизию. - какую дивизию? - вашу. девятую гвардейскую.', '- соедини.', '- хорошо! сегодня побываем там вместе.', '- у  нас  с  вами.', '- дайте ему трубку.', '- отменяется.', '- как? годится для радиопропаганды? - даже очень.', '- страшно? - так точно! - а на партактиве ты выступал смело.', '- нет! уверен! - хорошо. позвоню через час.', '- ты  уж  не  забирайся в первый раз высоко. - не  бойся,  мама!', '- ладно.', '- что ноги? - торчат. из гондолы...', '- нога...', '- попробую сесть на брюхо. может, получится... - пробуй. я погляжу...', '- как вы, михаил павлович.', '- а сколько в карманах?', '- еще есть для приятельницы... - покажите!', '- сейчас все расскажу...', '- видать, загрузло...', '- и что же с ними произошло? - да  что.', '- с зубановым. - нет, я выехал раньше. - а знаете, что он погиб?..', '- я  тоже  люблю проехаться с ветерком.', '- почитайте-ка.', '- хорошо.', '- солидный.', '- где убил? - сегодня ночью в ихнем окопе был.', '- хорошо.', '- сколько же?', '- запечатайте  его  в  конверт.', '- что вы сказали? - нервы,  сэр. - к  черту  твои  нервы. - заткнись.', '- прекрасно.', '- завтра, на третий день после вторжения.', '- хорошо. это замечательно. вы сами откуда? - из штата кентукки, сэр. - у вас хорошее отделение?, - лучшее в роте, сэр. - а солдаты роты думают также? - видите ли, сэр... - они глупцы, не правда ли?', '- много,  много.', '- конечно.', '- ищите.', '- слава  богу! власть делить - это хуже нету. - погодите  радоваться.', '- ох-хо-хо! как по-ихнему - руки вверх? - хенде хох. - точно.', '- продолжай наблюдение!', '- микстура. ну, спирт, ну?', '- холодно еще. - тулуп дадите?..', '- глупости не стоит делать даже со скуки.', '- ну,  держи  фрицевский. освоишь, мыслю я.', '- а,  леший!.. - немцы... где немцы?', '- из \"максима\"? - из \"максима\". и из других систем тоже. - вот  здорово!.. - а комнату тебе дадут? - дадут, дадут. - отдельную? - конечно. я ведь строевой командир. - мы  к  тебе  приедем. - кто это - мы?', '- разрешите присоединиться?', '- а-а...', '- я не боюсь.', '- черт! ну, где он может быть, этот склад?', '- не жахнут.', '- стекла  все повылетали. тебя как зовут-то? - фамилия - прижнюк. - эх, гранатку бы! - да,  вооружиться  всем.', '- они не обидятся!', '- бой  - для победы. - пустые  разговоры. - собрано уже. - воздух! штук двадцать бомбовозов! - ховайтесь,  хлопцы. - наблюдателю  остаться! они могут снова... - станкач волокут! сюда... - каски! каски надеть всем!..', '- хороший был пулеметчик. - товарищ лейтенант! тут гражданские!', '- идти можешь? - круги перед глазами.', '- у них у самих не густо.', '- сменяют. шило на мыло.', '- глубоко? - кажется, нет. поверху осколок.', '- одолжил, - усмехнулся пограничник, - то-то несладкий он.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hhai1ZXAYNH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8be5c21-cf5a-47ad-9f07-5d12642c657c"
      },
      "cell_type": "code",
      "source": [
        "q = []\n",
        "a = []\n",
        "\n",
        "fname = os.path.join(DIR, file[0])\n",
        "with open(fname, 'r', encoding='utf-8') as f:\n",
        "  subs = cleaned(f.read().lower())\n",
        "  for num in range(1, len(subs)):\n",
        "    q.append(subs[num-1])\n",
        "    a.append(subs[num])\n",
        "    \n",
        "    \n",
        "print(len(q), len(a))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39665 39665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-lzZG-VnXF7e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_seq2seq_files(questions, answers, path='', TESTSET_SIZE = 30000):\n",
        "    \n",
        "    # open files\n",
        "    train_enc = open(path + 'train.enc','w', encoding='utf-8')\n",
        "    train_dec = open(path + 'train.dec','w', encoding='utf-8')\n",
        "    test_enc  = open(path + 'test.enc', 'w', encoding='utf-8')\n",
        "    test_dec  = open(path + 'test.dec', 'w', encoding='utf-8')\n",
        "\n",
        "    # choose 30,000 (TESTSET_SIZE) items to put into testset\n",
        "    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n",
        "\n",
        "    for i in range(len(questions)):\n",
        "        try:\n",
        "            if i in test_ids:\n",
        "                test_enc.write(questions[i]+'\\n')\n",
        "                test_dec.write(answers[i]+ '\\n' )\n",
        "            else:\n",
        "                train_enc.write(questions[i]+'\\n')\n",
        "                train_dec.write(answers[i]+ '\\n' )\n",
        "            if i%10000 == 0:\n",
        "                print ('>> written {} lines'.format(i))\n",
        "        except UnicodeEncodeError:\n",
        "            print(questions[i], answers[i])\n",
        "\n",
        "    # close files\n",
        "    train_enc.close()\n",
        "    train_dec.close()\n",
        "    test_enc.close()\n",
        "    test_dec.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "266eyjKlXPsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9fcc3f86-49e5-4475-edc4-b518a2b25c3e"
      },
      "cell_type": "code",
      "source": [
        "prepare_seq2seq_files(q,a, path='./data', TESTSET_SIZE=1000)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> written 0 lines\n",
            ">> written 10000 lines\n",
            ">> written 20000 lines\n",
            ">> written 30000 lines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UkJLfyp6aA0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d7d97179-bf06-4d30-e81c-f7dcdfa3934c"
      },
      "cell_type": "code",
      "source": [
        "! ls -a"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".\t data\t       datatrain.dec  NLP_Datasets\t tensorflow_chatbot\n",
            "..\t datatest.dec  datatrain.enc  practical_seq2seq\n",
            ".config  datatest.enc  data_utils.py  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xb1io3WzaGHn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}